import 'dart:math';
import '../models/text_data.dart';
import '../models/analysis_result.dart';

/// –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
class WordFrequencyCounter {
  final Map<String, int> _frequency = {};
  final Set<String> _stopWords;

  WordFrequencyCounter({Set<String>? stopWords}) 
      : _stopWords = stopWords ?? _defaultStopWords;

  static final Set<String> _defaultStopWords = {
    '–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '–∑–∞', '–ø—Ä–∏',
    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
    'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did',
    'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall'
  };

  /// –î–æ–±–∞–≤–ª—è–µ—Ç —Å–ª–æ–≤–æ –≤ —Å—á–µ—Ç—á–∏–∫
  void addWord(String word) {
    final normalizedWord = _normalizeWord(word);
    if (normalizedWord.isNotEmpty && !_stopWords.contains(normalizedWord.toLowerCase())) {
      _frequency[normalizedWord] = (_frequency[normalizedWord] ?? 0) + 1;
    }
  }

  /// –î–æ–±–∞–≤–ª—è–µ—Ç –≤—Å–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
  void addText(String text) {
    final words = _extractWords(text);
    for (final word in words) {
      addWord(word);
    }
  }

  /// –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Å–ª–æ–≤–æ (—É–±–∏—Ä–∞–µ—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É)
  String _normalizeWord(String word) {
    return word.replaceAll(RegExp(r'[^\w\u0400-\u04FF]'), '').toLowerCase();
  }

  /// –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞
  List<String> _extractWords(String text) {
    return text.split(RegExp(r'\s+'))
        .where((word) => word.isNotEmpty)
        .toList();
  }

  /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É —Å–ª–æ–≤
  Map<String, int> get frequency => Map.unmodifiable(_frequency);

  /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø N —Å–ª–æ–≤
  List<String> getTopWords(int count) {
    final sortedEntries = _frequency.entries.toList()
      ..sort((a, b) => b.value.compareTo(a.value));
    return sortedEntries.take(count).map((e) => e.key).toList();
  }

  /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
  int get totalWords => _frequency.values.fold(0, (a, b) => a + b);

  /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
  int get uniqueWords => _frequency.length;
}

/// –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —á–∞—Å—Ç–æ—Ç—ã —Å–∏–º–≤–æ–ª–æ–≤
class CharacterFrequencyCounter {
  final Map<String, int> _frequency = {};

  /// –î–æ–±–∞–≤–ª—è–µ—Ç —Å–∏–º–≤–æ–ª –≤ —Å—á–µ—Ç—á–∏–∫
  void addCharacter(String char) {
    _frequency[char] = (_frequency[char] ?? 0) + 1;
  }

  /// –î–æ–±–∞–≤–ª—è–µ—Ç –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
  void addText(String text) {
    for (final char in text.runes) {
      addCharacter(String.fromCharCode(char));
    }
  }

  /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É —Å–∏–º–≤–æ–ª–æ–≤
  Map<String, int> get frequency => Map.unmodifiable(_frequency);

  /// –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø N —Å–∏–º–≤–æ–ª–æ–≤
  List<String> getTopCharacters(int count) {
    final sortedEntries = _frequency.entries.toList()
      ..sort((a, b) => b.value.compareTo(a.value));
    return sortedEntries.take(count).map((e) => e.key).toList();
  }
}

/// –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
class ReadabilityAnalyzer {
  /// –í—ã—á–∏—Å–ª—è–µ—Ç –∏–Ω–¥–µ–∫—Å –§–ª–µ—à–∞-–ö–∏–Ω–∫–µ–π–¥–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
  static double calculateFleschKincaid(String text) {
    final sentences = _countSentences(text);
    final words = _countWords(text);
    final syllables = _countSyllables(text);

    if (sentences == 0 || words == 0) return 0.0;

    final avgWordsPerSentence = words / sentences;
    final avgSyllablesPerWord = syllables / words;

    return 206.835 - (1.015 * avgWordsPerSentence) - (84.6 * avgSyllablesPerWord);
  }

  /// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
  static int _countSentences(String text) {
    return text.split(RegExp(r'[.!?]+')).where((s) => s.trim().isNotEmpty).length;
  }

  /// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
  static int _countWords(String text) {
    return text.split(RegExp(r'\s+')).where((w) => w.isNotEmpty).length;
  }

  /// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≥–æ–≤ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
  static int _countSyllables(String text) {
    final words = text.split(RegExp(r'\s+')).where((w) => w.isNotEmpty);
    int totalSyllables = 0;

    for (final word in words) {
      totalSyllables += _countSyllablesInWord(word);
    }

    return totalSyllables;
  }

  /// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≥–∏ –≤ —Å–ª–æ–≤–µ
  static int _countSyllablesInWord(String word) {
    final cleanWord = word.toLowerCase().replaceAll(RegExp(r'[^\w]'), '');
    if (cleanWord.isEmpty) return 0;

    int syllables = 0;
    bool previousWasVowel = false;

    for (final char in cleanWord.runes) {
      final isVowel = _isVowel(String.fromCharCode(char));
      if (isVowel && !previousWasVowel) {
        syllables++;
      }
      previousWasVowel = isVowel;
    }

    // –°–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –∏–º–µ—Ç—å –º–∏–Ω–∏–º—É–º –æ–¥–∏–Ω —Å–ª–æ–≥
    return max(1, syllables);
  }

  /// –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–∏–º–≤–æ–ª –≥–ª–∞—Å–Ω–æ–π
  static bool _isVowel(String char) {
    const vowels = 'aeiouy–∞–µ—ë–∏–æ—É—ã—ç—é—è';
    return vowels.contains(char.toLowerCase());
  }
}

/// –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
class TextAnalyzer {
  final Set<String> _stopWords;
  final bool _removeStopWords;
  final bool _calculateReadability;

  TextAnalyzer({
    Set<String>? stopWords,
    bool removeStopWords = true,
    bool calculateReadability = true,
  }) : _stopWords = stopWords ?? {},
       _removeStopWords = removeStopWords,
       _calculateReadability = calculateReadability;

  /// –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
  Future<AnalysisResult> analyzeText(TextData textData) async {
    print('üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç: ${textData.source}...');

    // –°–æ–∑–¥–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏
    final wordCounter = WordFrequencyCounter(stopWords: _stopWords);
    final charCounter = CharacterFrequencyCounter();

    // –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
    wordCounter.addText(textData.content);
    charCounter.addText(textData.content);

    // –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
    final mostCommonWords = wordCounter.getTopWords(20);

    // –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
    final longestWords = _findLongestWords(textData.content);

    // –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞
    final averageWordLength = _calculateAverageWordLength(textData.content);

    // –í—ã—á–∏—Å–ª—è–µ–º —á–∏—Ç–∞–µ–º–æ—Å—Ç—å
    final readabilityScore = _calculateReadability 
        ? ReadabilityAnalyzer.calculateFleschKincaid(textData.content)
        : 0.0;

    // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    final additionalMetrics = {
      'sentenceCount': _countSentences(textData.content),
      'paragraphCount': _countParagraphs(textData.content),
      'averageSentenceLength': _calculateAverageSentenceLength(textData.content),
      'lexicalDiversity': _calculateLexicalDiversity(wordCounter.frequency),
    };

    final result = AnalysisResult(
      textId: textData.id,
      source: textData.source,
      wordFrequency: wordCounter.frequency,
      characterFrequency: charCounter.frequency,
      mostCommonWords: mostCommonWords,
      longestWords: longestWords,
      averageWordLength: averageWordLength,
      uniqueWords: wordCounter.uniqueWords,
      totalWords: wordCounter.totalWords,
      readabilityScore: readabilityScore,
      additionalMetrics: additionalMetrics,
      analysisTime: DateTime.now(),
    );

    print('‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω: ${result.uniqueWords} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤, —á–∏—Ç–∞–µ–º–æ—Å—Ç—å: ${readabilityScore.toStringAsFixed(1)}');
    return result;
  }

  /// –ù–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–µ –¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
  List<String> _findLongestWords(String text) {
    final words = text.split(RegExp(r'\s+'))
        .where((w) => w.isNotEmpty)
        .map((w) => w.replaceAll(RegExp(r'[^\w\u0400-\u04FF]'), ''))
        .where((w) => w.isNotEmpty)
        .toList();

    words.sort((a, b) => b.length.compareTo(a.length));
    return words.take(10).toList();
  }

  /// –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞
  double _calculateAverageWordLength(String text) {
    final words = text.split(RegExp(r'\s+'))
        .where((w) => w.isNotEmpty)
        .map((w) => w.replaceAll(RegExp(r'[^\w\u0400-\u04FF]'), ''))
        .where((w) => w.isNotEmpty)
        .toList();

    if (words.isEmpty) return 0.0;

    final totalLength = words.fold(0, (sum, word) => sum + word.length);
    return totalLength / words.length;
  }

  /// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
  int _countSentences(String text) {
    return text.split(RegExp(r'[.!?]+')).where((s) => s.trim().isNotEmpty).length;
  }

  /// –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤
  int _countParagraphs(String text) {
    return text.split('\n\n').where((p) => p.trim().isNotEmpty).length;
  }

  /// –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
  double _calculateAverageSentenceLength(String text) {
    final sentences = _countSentences(text);
    final words = text.split(RegExp(r'\s+')).where((w) => w.isNotEmpty).length;
    return sentences > 0 ? words / sentences : 0.0;
  }

  /// –í—ã—á–∏—Å–ª—è–µ—Ç –ª–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ
  double _calculateLexicalDiversity(Map<String, int> wordFrequency) {
    final totalWords = wordFrequency.values.fold(0, (a, b) => a + b);
    if (totalWords == 0) return 0.0;
    return wordFrequency.length / totalWords;
  }
}
